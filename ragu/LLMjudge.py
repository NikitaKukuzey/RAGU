from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch


def ask_llm_qwen_local(answer1: str, answer2: str, llm_pipe) -> float:
    prompt = (
        f"""
РОЛЬ
Ты — ИИ-анализатор, который определяет степень фактического пересечения двух текстов. Твоя задача — вычислить, какая доля фактов из первого текста содержится во втором, и выдать результат в виде одного числа.

ИНСТРУКЦИЯ
Мысленно разбей "Ответ 1" на отдельные атомарные факты. Это общее количество фактов (Y).

Для каждого факта из "Ответа 1" проверь, присутствует ли он семантически в "Ответе 2". Это количество найденных фактов (X).

Рассчитай пропорцию X / Y.

Выдай в качестве ответа только полученное число, округленное до двух знаков после запятой.
Выдавай только число и ничего лишнего.

ПРАВИЛА СРАВНЕНИЯ
Считать фактом, если это минимальная, неделимая единица информации.

Считать найденным, если факт подтверждается в "Ответе 2" прямо, синонимами, через перефраз, как прямое логическое следствие или при любом изменении порядка слов, которое не меняет смысл утверждения.

Считать не найденным, если факт отсутствует или противоречит информации в "Ответе 2".

ФОРМАТ ОТВЕТА
Твой ответ должен быть СТРОГО ОДНИМ ЧИСЛОМ от 0.00 до 1.00.

Не добавляй никаких объяснений, комментариев, текста или знака процента.

ПРИМЕРЫ
Ответ 1: "Париж - столица Франции. Он стоит на реке Сена." (2 факта)

Ответ 2: "Главный город Франции, Париж, расположен на Сене." (2 из 2 фактов найдены)

Результат: 1.00

Ответ 1: "Земля - третья планета от Солнца, у нее один спутник - Луна." (2 факта)

Ответ 2: "Земля является третьей планетой в Солнечной системе." (1 из 2 фактов найден)

Результат: 0.50

Ответ 1: "Самая высокая гора - Эверест." (1 факт)

Ответ 2: "Джомолунгма находится в Гималаях." (0 из 1 факта найдено, т.к. не сказано, что она самая высокая)

Результат: 0.00

НОВЫЙ ПРИМЕР:

Ответ 1: "Мороженое съели." (1 факт)

Ответ 2: "Съели мороженое." (1 из 1 факта найдено)

Результат: 1.00

ЗАДАЧА
Ответ 1: {answer1}
Ответ 2: {answer2}
Результат:
"""
    )
    generated_text = llm_pipe(prompt)[0]['generated_text'][:5].strip()
    print(generated_text)

    try:
        result = float(generated_text)
        print(f"Сгенерированный результат: {result}")
        if result != 0.0:
            result = 1
        return result
    except ValueError:
        print(f"Ошибка: Не удалось преобразовать '{generated_text}' в число. Возвращаем 0.0.")
        return -1


def compare_llm_answers_with_llm(list1: list[str], list2: list[str]) -> float:
    model_name = "Qwen/Qwen2.5-7B-Instruct"
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="cuda",
        torch_dtype="auto",
        trust_remote_code=True
    )

    llm_pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=6,  # Ожидаем короткий числовой ответ (например, "0.50")
        temperature=0.3,
        do_sample=False,
        return_full_text=False  # Возвращает только сгенерированный текст, без входного промпта
    )

    if len(list1) != len(list2):
        raise ValueError("Списки должны быть одинаковой длины")

    results = []
    for a, b in zip(list1, list2):
        result = ask_llm_qwen_local(a, b, llm_pipe)
        if result == -1:
            continue
        results.append(result)

    return sum(results) / len(results) if results else 0.0


